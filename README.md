â™Ÿï¸ Self-Trained Chess AI (Neural Network + MCTS)

A research-oriented chess engine that learns to play chess through self-play reinforcement learning, using a policyâ€“value neural network combined with Monte Carlo Tree Search (MCTS).

This project focuses on how chess AIs are trained, evaluated, and stabilized, not just raw playing strength.

ğŸš€ Key Features

ğŸ§  Policy + Value Neural Network (AlphaZero-style)

ğŸŒ² Monte Carlo Tree Search (MCTS) for move selection

ğŸ” Self-Play Reinforcement Learning

âš”ï¸ Arena Evaluation (model vs model testing)

ğŸ§ª Draw / Mode-Collapse Detection & Mitigation

ğŸ”Œ UCI-compatible engine (usable in chess GUIs)

âš¡ GPU-accelerated training (CUDA supported)

ğŸ“Š Current Training Status (Important Note)

This project has completed multiple self-play training iterations.

Observed behavior

Early iterations show clear improvement

Later iterations converge toward a draw-dominant equilibrium

Arena evaluation between distant checkpoints often results in draws

Example

selfplay_iter_18 vs selfplay_iter_10 â†’ 20 / 20 draws


This indicates policy convergence, a known phenomenon in self-play RL,
not a bug or failure.

Breaking this equilibrium typically requires:

Larger neural networks

Much higher self-play volume

External data (e.g. master games / PGNs)

Stronger exploration or curriculum learning

ğŸ§  Architecture Overview
Neural Network

Input: (18, 8, 8) board encoding

Backbone: Convolutional layers + residual blocks

Outputs:

Policy head: move probabilities (4096 possible moves)

Value head: position evaluation in range [-1, 1]

Inspired by AlphaZero-style policy/value learning, implemented fully from scratch.

ğŸ“ Project Structure
chess-ai-mcts/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ board_encoder.py        # Board â†’ tensor encoding
â”‚   â”œâ”€â”€ network/
â”‚   â”‚   â””â”€â”€ chess_net.py            # Policyâ€“value neural network
â”‚   â”œâ”€â”€ engine/
â”‚   â”‚   â”œâ”€â”€ mcts.py                 # Monte Carlo Tree Search
â”‚   â”‚   â””â”€â”€ chess_uci_engine.py     # UCI-compatible engine
â”‚   â””â”€â”€ training/
â”‚       â”œâ”€â”€ self_play.py            # Self-play game generation
â”‚       â”œâ”€â”€ trainer.py              # Training loop
â”‚       â””â”€â”€ anti_collapse_self_play.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_loop.py               # Main training loop
â”‚   â”œâ”€â”€ arena_eval.py               # Model vs model evaluation
â”‚   â””â”€â”€ play_vs_stockfish.py
â”‚
â”œâ”€â”€ checkpoints/                    # Saved model checkpoints
â””â”€â”€ requirements.txt

ğŸ› ï¸ Installation
Requirements

Python 3.9+

PyTorch

python-chess

NumPy

pip install -r requirements.txt


Verify setup:

python -c "import torch, chess; print('Setup OK')"

ğŸ” Training (Self-Play)

Run the full self-play + training loop:

python -m scripts.train_loop

What happens internally

Current model plays games against itself using MCTS

Positions, policies, and values are collected

Neural network is trained on generated data

New checkpoint is saved

Process repeats

âš”ï¸ Arena Evaluation (Model vs Model)

Compare two trained checkpoints:

python -m scripts.arena_eval \
  --candidate checkpoints/selfplay_iter_18.pt \
  --baseline  checkpoints/selfplay_iter_10.pt

Arena rules

Randomized colors

Early-game exploration

Max move limit

Resign logic based on value head

Used to measure true improvement vs draw equilibrium.

ğŸ§ª Anti-Collapse Measures Implemented

To reduce draw spirals and training stagnation:

Opening diversity

Temperature scheduling

Early resignation thresholds

Draw value penalties

Repetition awareness

Reduced maximum game length

All measures are conservative, prioritizing training stability.

ğŸ® UCI Engine Usage

Run the engine in UCI mode (for GUIs like Arena, CuteChess, etc.):

python src/engine/chess_uci_engine.py checkpoints/selfplay_iter_18.pt


You can then add it as an engine in any UCI-compatible chess GUI.

âš ï¸ Limitations (Honest & Transparent)

âŒ Not competitive with Stockfish or Leela

âš ï¸ Strength limited by compute and training volume

âš–ï¸ Self-play equilibrium reached early

ğŸ§ª Lichess bot integration is experimental

This is a research & learning project, not a production chess engine.

ğŸ§  What This Project Demonstrates

Strong understanding of reinforcement learning loops

Practical implementation of MCTS

Handling self-play instability

Debugging mode collapse

Building scalable ML training pipelines

Skills directly relevant to:

Game AI

Reinforcement learning research

ML / systems engineering roles

ğŸ”® Future Improvements

Larger neural networks

External PGN bootstrapping

Parallel self-play

Curriculum learning

Opening books

Endgame tablebases

ğŸ“œ License

MIT License â€” free to use, modify, and learn from.

ğŸ™Œ Acknowledgements

Inspired by:

DeepMindâ€™s AlphaZero

Stockfish NNUE ideas

python-chess library